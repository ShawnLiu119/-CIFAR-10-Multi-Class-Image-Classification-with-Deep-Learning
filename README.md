# Multi-Class-Image-Classification-with-Deep-Learning
Multi-Class Image Classification with Deep Learning using CNN / DNN with hyperparameter tuning


### Abstract
Multi-class image classification has been one of application areas under computer vision domain where deep learning has achieved great progress in last decade together with object detection and image segmentation. As one of deep learning model structure, Convolutional Neural Network (CNN) has been proved to outperform other models especially in image classification because of its advantageous capability to extract features from the input images and evolve based on self-learning. In this research, the CIFAR dataset, a collection of color images evenly distributing across 10 class objects including bird, cat, automobile and so on, has been used to training and testing of multiple DNN and CNN models. The primary objective of this research is to compare the performance of CNN versus DNN on image classification, to experiment with differentiated model structure in terms of number of layers and regularization, and to explore how number of layers and regularization such as dropout and batch normalization impact the model performance.

### Introduction
Visual object recognition has been one of focused areas where a lot of artificial intelligence studiers attempt to train machine to mimic human being capability of detecting and recognizing images which vary in position, scale, pose and so on. Recently, deep neural networks have achieved impressive progress in this area, while CNN outperforms others due to its advantageous capability of extracting features from images through self-learning (Tien 2018). In the research, several experiments are conducted to experiment with neural network including DNN and CNN on CIFAR-10 dataset as shown in Figure 1, which is one of the well-known dataset of natural images.
Figure
![image](https://user-images.githubusercontent.com/43327902/185470378-aaebe12b-1f55-499f-8a75-f92ef1c21d48.png)

The main object of conducting these experiments is to make fair comparison between Convolutional Neural Network (CNN) against typical Dense Neural Network (DNN) to explore how specialized feature learning capabilities enable it to excel in image recognition. While at a more granular level, multiple model structures with variety of number of layers including convolutional and pooling layers are experimented to explore the impact on model performance. Additionally, regularization such as dropout and batch normalization, and data augmentation are applied on the base model to testify whether these add-on can help improve model performance.

### Literature Review
Please refer to the final report (PDF) in the repo

### Method

##### Data Collection & Exploration
The CIFAR-10 dataset consists of 60,000 32X32 color images, which evenly distributed with 6000 instances across 10 classes. The dataset has been split into training set with 50,000 images and 10,000 test images. The classes are completely mutually exclusive.

##### Data Preprocessing
Data normalization is a necessary step to take before the image data is fed into classification training. Given all pixels are in color-scale ranging from 0 to 255, the data is normalized to 0 to 1 simply by dividing all pixel data by 255. The dataset, which is by default in 4 dimension, has been reshaped to flat array for DNN model training.

##### Model Construction and Training
Ten neural network models have been constructed with Keras from TensorFlow and experimented for prediction after being trained with the whole dataset, conclusional insights are generated based on model performance comparison across models with different structure such as DNN vs CNN, different number of layers, regularization and so on with more details shown in Figure 2. As top priority, DNN and CNN models are developed and experimented in multiple structures varying by number of layers and regularization. As baseline, Model 1 and 2 are DNN models with 2 and 3 dense layers, to compare against two CNN models (Model 3 and 4) with same number of layers but using convolutional and pooling layer instead. None of models have factored in any regularization components. Through Model 5 to 8, additional model structure has been added on the baseline model including the Dropout layers with dropout ratio 0.3 after each DNN or CNN pooling layer, and Batch Normalization before final classification dense layer in order to reduce overfitting problems, which is commonly seen in neural network training process. L2 regularization with learning rate 0.001 is also added. With added layers of dropout and batch normalization and classification regularization, we can compare the set of models with model 1/2/3/4 by setting everything else the same and see how these regularization tactics can help further reduce overfitting and improve the prediction accuracy.

![image](https://user-images.githubusercontent.com/43327902/185470845-7e20551b-9f05-4860-b197-d653d51996de.png)

Selected hyperparameters are utilized consistently to compile the model structure across all CNN models. For example, in convolutional layer, kernel_size is 3 by 3 with stride 1 to screen the image and extract features. To reduce the computation complexity, max pooling 2 by 2 are built in with stride 2 after each convolutional layer. The number of filters for first convolutional layer is set as 128, 256 for the second convolutional layer, and 512 for the third. Relu is used as activation function for each layer, while Softmax is used in final dense layer as this is a multi-classification problem, so does SparseCategoricalCrossEntropy as loss function. Adam is deployed as optimizer. For comparison purpose, all models are trained on 100 epochs and 512 instances as batch size, while the actual training epochs might vary as early stopping is set with patience 3 to reduce computational time. After convolutional and pooling process has been done with the data, a dense layer with 384 neurons is inserted before the final dense layer with 10 neurons with each associated with one particular image class. Another technique utilized in this research is data augmentation, which has proved to be capable of increasing the diversity of training set by applying random transformation such as shifting and rotation the image. Model 9 and 10 are derived based on CNN models from Model 3 and 4 with two additional data augmentation layers added before the first convolution layer. Two data augmentation layers are RandomFlip and RandomRotation with rate 0.2, so the input data is transformed and manipulated before it is fed into first convolutional layer for feature extraction. Another experiment that has been conducted is to run Gridsearch Cross Validation, which specifically focus on optimal hyperparameters of CNN model. To balance out the computation cost, I decided to testify pooling type (max, average), activation function for convolutional layer (sigmoid, tanh, relu), kernel_size ((3,3), (5,5)), and optimizer (‘adam’ , ‘rmsprop’) as shown in Figure 3. To facilitate the search process to land on the conclusion faster, the model with each hyperparameter combination is set to be trained trained on 10 epochs with cross validation in three folds.

##### Result
Experimental models are evaluated with performance metrics being traced, including accuracy, F1 score, recall, precision and computing time, as summarized in Figure 4. Overall, CNN models outperform DNN models with significant higher accuracy gain across training, validation and testing, although the training of CNN models is more computation expensive. The two CNN models from Model 3 and 4 have gained above 85% training accuracy, and above 70% validation and testing accuracy while the two DNN models from Model 1 and 2 only get to close to 50% as shown in figure below.
![image](https://user-images.githubusercontent.com/43327902/185471032-e557faaa-f1e7-4474-95ef-f10024552bbe.png)

Model 8 (CNN: 3 conv2d, 3 max pooling, 3 dropout, 1 batch normalization, with L2 regularization) delivers best performance across all models. The model has achieved 96.55% training accuracy, 80.08% on validation and 79.88% on test dataset, achieving 4% additional accuracy gain with additional one convolutional and pooling layer. The discrepancy of accuracy score between training and validation/testing suggest that overfitting problem still exist to some extent.Interestingly, if we just look at CNN models, the ones with three layers have performed better than the ones with two layers. Model 4 (CNN: 3 conv2d, 3 max pooling) has achieved 72.89% test accuracy while Model 3 (CNN: 2 conv2d, 2 max pooling) has classified 71.04% of images, the same case applies for Model 7 (CNN: 2 conv2d, 2 max pooling, 2 dropout, 1 batch normalization, with L2 regularization) and Model 8. Another observation is that, after adding dropout and batch normalization layers, the performance has been improved with higher accuracy scores, with evidence that Model 8 has achieved higher accuracy score than Model 4 which has similar fundamental CNN model structure but without regularization layers including dropout and batch normalization. However, this trend has been completely opposite for DNN models. With regularization is added, the test accuracy achieved by Model 5 (43.45%) and 6 (41.63%) have been worse than their baseline peers, Model 1 (47.44%) and 2 (46.22%). This is might be attributed to the information loss brought by feature dropout as our model structure is relatively shallow, which should be investigated in deeper level.

![image](https://user-images.githubusercontent.com/43327902/185471142-3b735d24-11c4-4f7e-b212-9b0cf8a5a997.png)

Model 9 and 10, which have added additional data augmentation layers into CNN baseline models – Model 3 and 4 did not deliver better performance as expected originally. Model 9 (CNN: 2 conv2d, 2 max pooling, 1 RandomFlip, 1 RandomRotation) achieves 58.8% test accuracy, while Model 10 (CNN: 3 conv2d, 3 max pooling, 1 RandomFlip, 1 RandomRotation) delivers 56.34% test accuracy only as shown in Figure 6. However, the data augmentation does help reduce overfitting problem. For example, Model 9, which is based on Model 3 as baseline CNN, has only 1.5% accuracy difference between training and test, while Model 3 has approximately 20% (91.73% training, 71.04% test) as shown in Figure 7.

![image](https://user-images.githubusercontent.com/43327902/185471274-41cddabb-7074-4655-bd82-040e0450a174.png)
![image](https://user-images.githubusercontent.com/43327902/185471318-07e6fe77-2a2a-4b69-b074-31509d9bea2f.png)
![image](https://user-images.githubusercontent.com/43327902/185471367-0e8b4a8f-968b-4ce8-a7af-698760fb961f.png)

### Conclusion

Overall, CNN outperforms DNN specifically in image classification field thanks to its extraordinary capability to extract global and local features based on self-learning. Performance could be optimized and improved potentially by adding more convolutional layers and filters to extract more precise information from images, but at the same time might bring overfitting problem and increase computational complexity. Factoring regularization techniques such as adding dropout layers and batch normalization could potentially help mitigate these problems. Data augmentation could be another option to reduce overfitting, however it has not been proved is help improve model performance in this study. CNN model could definitely be utilized for face recognition application especially for maximizing its advantage of high accuracy on detecting features and classifying images. However, there might be some challenge with applying it on real time application due to its computational complexity.





